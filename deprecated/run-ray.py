"""A simple multi-agent env with two agents play rock paper scissors.

This demonstrates running two learning policies in competition, both using the same
RLlib algorithm (PPO by default).

The combined reward as well as individual rewards should roughly remain at 0.0 as no
policy should - in the long run - be able to learn a better strategy than chosing
actions at random. However, it could be possible that - for some time - one or the other
policy can exploit a "stochastic weakness" of the opponent policy. For example a policy
`A` learns that its opponent `B` has learnt to choose "paper" more often, which in
return makes `A` choose "scissors" more often as a countermeasure.
"""

import re

import wandb
from ray.rllib.connectors.env_to_module import (
    AddObservationsFromEpisodesToBatch,
    FlattenObservations,
    WriteObservationsToEpisodes,
)
from ray.rllib.core.rl_module.marl_module import MultiAgentRLModuleSpec
from ray.rllib.core.rl_module.rl_module import SingleAgentRLModuleSpec
from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv
from ray.rllib.utils.test_utils import (
    add_rllib_example_script_args,
    run_rllib_example_script_experiment,
)
from ray.tune.registry import get_trainable_cls, register_env
import numpy as np
from rideshare.env_multi import MultiRideshareEnv

env_fn = MultiRideshareEnv

OD = [[0., 0.3, 0.7],
      [0.4, 0., 0.6],
      [0.1, 0.9, 0.]]
C = [[0., 4., 5.],
     [4., 0., 2.,],
     [5., 2., 0.]]
init_passenger_distribution = [2000., 3000., 5000.]
init_driver_distribution = [1000., 5000., 2000.]
# Set vector_state to false in order to use visual observations (significantly longer training time)
ts = 2*864000
env_kwargs = {
    "OD":                                       np.array(OD),
    "C":                                        np.array(C),
    "init_passenger_distribution":              np.array(init_passenger_distribution),
    "init_driver_distribution":                 np.array(init_driver_distribution),
    "change_range":     1,                      # only changes 1 per second
    "max_rate":         30.,                    # maximum charged per mile is 30
    "max_timestep":     ts ,                    # seconds in a day
    "lbd":              3.,                     # lambda
    "rp":               12.,                    # maximum rate public transportation charges
    "g":                5.,                     # gas cost
    "num_D_samples":    10,                     # random samples generated by drivers
    "a_d":              1e-1,                   # at each second, can only change very little.
    "p_d":              1e-1,                   # at each second, passengers can deviate by.
    "alpha":            1e-4,                   # step parameter for continuous term
    "ppo_args":         {
                            "steps": ts,
                            "gamma": 0.99,
                        }
}
run = wandb.init(
    # Set the project where this run will be logged
    project="Rideshare Multi",
    # Track hyperparameters and run metadata
    config=env_kwargs,
    # sync_tensorboard=True,
    mode="disabled"
)


parser = add_rllib_example_script_args(
    default_iters=50,
    default_timesteps=ts,
)


register_env(
    "multirideshare_v0",
    lambda _: ParallelPettingZooEnv(env_fn(**env_kwargs)),
)


if __name__ == "__main__":
    args = parser.parse_args()

    assert args.num_agents == 2, "Must set --num-agents=2 when running this script!"
    assert (
        args.enable_new_api_stack
    ), "Must set --enable-new-api-stack when running this script!"
    # config.experimental(_enable_new_api_stack=True)
    base_config = (
        get_trainable_cls(args.algo)
        .get_default_config()
        .environment("multirideshare_v0")
        .rollouts(
            env_to_module_connector=lambda env: (
                AddObservationsFromEpisodesToBatch(),
                FlattenObservations(multi_agent=True),
                WriteObservationsToEpisodes(),
            ),
        )
        .experimental(_enable_new_api_stack=True)
        .multi_agent(
            policies={"p0", "p1"},
            # `player_0` uses `p0`, `player_1` uses `p1`.
            policy_mapping_fn=lambda aid, episode: re.sub("^player_", "p", aid),
        )
        .training(
            vf_loss_coeff=0.005,
        )
        .rl_module(

            rl_module_spec=MultiAgentRLModuleSpec(
                module_specs={
                    "p0": SingleAgentRLModuleSpec(),
                    "p1": SingleAgentRLModuleSpec(),
                }
            ),
        )
    )

    run_rllib_example_script_experiment(base_config, args)